# Projeto Final - Extra√ß√£o de Dados I
> *Turma 11080 - Santander Coders 2024 - Engenharia de Dados*

Desenvolvimento do projeto "Sistema de Monitoramento de Avan√ßos no Campo da Gen√¥mica" com o intuito de extrair dados da [NewsAPI](https://newsapi.org/) relacionados ao campo da g√™nomica e persist√≠-los, sem nenhum tipo de tratamento a cada 1 hora, com o processo de Cargas em Batches.
Tamb√©m √© simulado um sistema de mensageria usando Kafka, onde dados s√£o produzidos (Producer), consumidos (Consumer) e persistidos no mesmo local do item anterior.
Por fim, esses dados s√£o tratados e persistidos em outro local para consulta do p√∫blico final.

**Todo projeto foi desenvolvido com a linguagem de programa√ß√£o Python no Databricks Community.**

## ‚úíÔ∏èAutores 
- [Alessandra Cruz](https://github.com/alessandracruz)
- [√Ålex Buracosky](https://github.com/aburacosk)
- [Diana Osorio](https://github.com/diana468)
- [Diogo Moura](https://github.com/HyogoMoura)
- [Felipe Zanardo](https://github.com/FelipeBZanardo)
- [Thiago Silva](https://github.com/thiagodemedeiros)

## üìì Acesso direto aos notebooks no Databricks:

- [Cargas em Batches.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825366/5346947633260273/latest.html)
- [Configura√ß√£o Kafka.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825384/5346947633260273/latest.html)
- [Consulta final.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/3638016907884296/5346947633260273/latest.html)
- [Consumidor - Kafka.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825397/5346947633260273/latest.html)
- [Produtor - Kafka.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825393/5346947633260273/latest.html)
- [Servidor.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825389/5346947633260273/latest.html)
- [T√≥pico - Kafka.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/1937876184825391/5346947633260273/latest.html)
- [Usado apenas na apresenta√ß√£o.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4227649743692003/2977122825521158/5346947633260273/latest.html)

## üìã Enunciado do Projeto

### **Sistema de Monitoramento de Avan√ßos no Campo da Gen√¥mica**

#### Contexto:
>O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em gen√¥mica e pesquisa de medicina personalizada. A gen√¥mica √© o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biom√©dica. Permite a an√°lise do DNA para identificar variantes gen√©ticas e muta√ß√µes associadas a doen√ßas e facilita a personaliza√ß√£o de tratamentos com base nas caracter√≠sticas gen√©ticas individuais dos pacientes.

>A empresa precisa se manter atualizada sobre os avan√ßos mais recentes na gen√¥mica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tend√™ncias em gen√¥mica que podem influenciar estrat√©gias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as √∫ltimas not√≠cias relacionadas √† gen√¥mica e √† medicina personalizada, e tamb√©m estuda o avan√ßo do campo nos √∫ltimos anos.

>O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confi√°vel e est√°vel. As principais atividades s√£o:

#### 1. Consumo de dados com a News API:
>Implementar um mecanismo para consumir dados de not√≠cias de fontes confi√°veis e especializadas em gen√¥mica e medicina personalizada, a partir da News API:
https://newsapi.org/

#### 2. Definir Crit√©rios de Relev√¢ncia:
>Desenvolver crit√©rios precisos de relev√¢ncia para filtrar as not√≠cias. Por exemplo, o time pode se concentrar em not√≠cias que mencionem avan√ßos em sequenciamento de DNA, terapias gen√©ticas personalizadas ou descobertas relacionadas a doen√ßas gen√©ticas espec√≠ficas.

#### 3. Cargas em Batches:
>Armazenar as not√≠cias relevantes em um formato estruturado e facilmente acess√≠vel para consultas e an√°lises posteriores. Essa carga deve acontecer 1 vez por hora. Se as not√≠cias extra√≠das j√° tiverem sidos armazenadas na carga anterior, o processo deve ignorar e n√£o armazenar as not√≠cias novamente, os dados carregados n√£o podem ficar duplicados.

#### 4. Dados transformados para consulta do p√∫blico final
>A partir dos dados carregados, aplicar as seguintes transforma√ß√µes e armazenar o resultado final para a consulta do p√∫blico final:
4.1 - Quantidade de not√≠cias por ano, m√™s e dia de publica√ß√£o;
4.2 - Quantidade de not√≠cias por fonte e autor;
4.3 - Quantidade de apari√ß√µes de 3 palavras chaves por ano, m√™s e dia de publica√ß√£o (as 3 palavras chaves ser√£o as mesmas usadas para fazer os filtros de relev√¢ncia do item 2 (2. Definir Crit√©rios de Relev√¢ncia)).
**Atualizar os dados transformados 1 vez por dia.**

Al√©m das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando √© necess√°rio, para isso foi desenhado duas op√ß√µes:

#### Op√ß√£o 1 - Apache Kafka e Spark Streaming:
>Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda n√£o foram armazenados no destino (os dados carregados n√£o podem ficar duplicados). E por fim, eliminar os dados tempor√°rios ap√≥s a verifica√ß√£o e a eventual carga.

#### Op√ß√£o 2 - Webhooks com notifica√ß√µes por eventos:
>Configurar um webhook para adquirir as √∫ltimas not√≠cias a partir de um evento representado por uma requisi√ß√£o POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda n√£o foram armazenados no destino (os dados carregados n√£o podem ficar duplicados). E por fim, eliminar os dados tempor√°rios ap√≥s a verifica√ß√£o e a eventual carga.

**Atividades que precisam ser realizadas pelo grupo definido em aula.**
>O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Op√ß√£o 1 e Op√ß√£o 2 para desenvolvimento.

## üìù Descri√ß√£o do Projeto

#### 1. Palavras-chave escolhidas:
- Outubro Rosa;
- DNA;
- Doen√ßa gen√©tica;
- Terapia gen√©tica.

>**Para melhor performance na busca das palavras-chave na NewsAPI tamb√©m foram pesquisadas no idioma ingl√™s com o aux√≠lio da biblioteca [Translate](https://pypi.org/project/translate/)**.

#### 2. Busca de dados por eventos:
Foi escolhida a op√ß√£o 1 - Apache Kafka e Spark Streaming.

>**Houve dificuldades com o Spark Streaming na hora de persistir os dados. Portanto, o Consumer Kafka foi desensolvido com a desserializa√ß√£o dos dados**.

#### 3. Orquestra√ß√£o de fluxos de dados:
Foi escolhido a bibilioteca Schedule.

#### 4. Persist√™ncia dos dados:
A persist√™ncia dos dados, tanto dos dados vindos diretamente da NewsAPI quanto os dados tratados, foi feita atrav√©s de arquivos parquet.

## üì∫ Demonstra√ß√£o

<p align="center">
  <img src="./_captures/Demonstracao.gif">
</p>

## ‚òëÔ∏è  Pr√©-requisitos
- Cadastro no **[Databricks Community](https://www.databricks.com/try-databricks#account)**;



## ‚öôÔ∏è Passo a passo para executar o projeto:
1. Cadastro e login no **[Databricks Community](https://community.cloud.databricks.com/login.html)**;
2. Import de todos os notebooks presentes nesse projeto para o workspace do Databricks:
    - Pode baixar todos os notebooks e importar via browse;
    - Ou pode importar atrav√©s da URL dos notebooks presentes no item [Acesso direto aos notebooks no Databricks](#-acesso-direto-aos-notebooks-no-databricks);
3. Criar e iniciar o cluster no Databricks;
4. Inicializar os notebooks 'Configura√ß√£o Kakfa', 'Servidor', 'T√≥pico - Kafka' com 'Run All';
5. Inicializar o notebook 'Cargas em Batches' com 'Run All' e verificar os dados a serem extra√≠dos e persistidos no arquivo parquet cujo path √© '/Filestore/projeto/dados_raw' com ajuda do notebook 'Usado apenas na apresenta√ß√£o';
6. Inicializar o notebook 'Consumidor-Kafka' com 'Run All' e em sequ√™ncia inicializar o notebook 'Produtor-Kafka' com 'Run All'. Verificar os dados saindo do produtor e chegando no consumidor;
7. Inicializar o notebook 'Consulta final' com 'Run All'. Nessa etapa os dados s√£o limpos e persistidos no arquivo parquet cijo path √© '/Filestore/projeto/dados_final/dados_completos'. Com ajuda do notebook 'Usado apenas na apresenta√ß√£o' √© poss√≠vel ver os dados persistidos.

>**Assistir o v√≠deo de demonstra√ß√£o do projeto!**

## üõ†Ô∏è Tecnologias Utilizas

* [Databricks Community](https://www.databricks.com/try-databricks#account)
* [Python](https://www.python.org/) - Linguagem de Programa√ß√£o
* [NewsAPI](https://newsapi.org/) - API para extra√ß√£o de dados de not√≠cias
* [Kafka](https://pypi.org/project/kafka-python/) - Servi√ßo de mensageria
* [Translate](https://pypi.org/project/translate/) - Usado para traduzir textos em python
* [Schedule](https://pypi.org/project/schedule/) - Usado para orquestra√ß√£o do fluxo de dados

## üö® Dificuldades
- Persist√™ncia dos dados no Consumer Kafka usando Spark Streaming. Por esse motivo acabou n√£o sendo utilizado;
- Mapeamento da estrutura dos dados vindos da NewsAPI. A coluna 'source' era composta de outras duas colunas 'id' e 'name';
- Tradu√ß√£o das palavras usando a biblioteca [Translate](https://pypi.org/project/translate/). Algumas palavras como 'title' simplesmente n√£o foi traduzida.
- Uso de orquestradores de fluxo de dados:
    - A biblioteca [Prefect](https://docs.prefect.io/3.0/get-started/install) explicada em aula apresenta erros ao ser usada no [Databricks Community](https://www.databricks.com/try-databricks#account);
    - Por esse motivo foi utilizada a biblioteca [Schedule](https://pypi.org/project/schedule/) extremamente simples para utiliza√ß√£o.

## üìà Melhorias futuras:
- Aumentar o n√∫mero de palavras-chave para aumentar as not√≠cias extra√≠das da [NewsAPI](https://newsapi.org/);
- Melhorar o consumo dos dados da [NewsAPI](https://newsapi.org/) j√° que o plano gratuito permite apenas 100 requisi√ß√µes ao dia;
- Fazer o deploy do projeto para n√£o precisar abrir v√°rias guias do Databricks no navegador.



